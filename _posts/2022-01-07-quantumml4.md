---
title: \[Quantum Data Science\] Part 4. Quantum Kernel Space
categories: [Quantum]
tags: [Quantum Computer]
excerpt: Quantum feature space using quantum kernel
sidebar:
  - title: "Quantum Computing"
    image: /assets/img/quantum.png
    image_alt: "image"
    nav: Quantum_eng
author_profile: False
---

$\def\ket#1{\mid #1 \rangle}\def\bra#1{\langle #1 \mid}$



## 1. Reproducing Kernel Hilbert Space

 In the linear regression, we can improve the predictive accuracy of the linear model by mapping original feature into higher dimension. For example, with $y = \beta_0 + x\beta_1$, we can reduce the mean square error further by adding higher term such as $x^2 , x^3,\cos x , \sin x,...$. If we keep add the term, we can shrink the error almost zero for most of regression problems. This is most basic method to build non-linear model.

 We can connect the above situation with Taylor expansion. A lot of functions in real world could be analyzed with Taylor expansion which means that we can analyze some function $f$ as $f(x) \approx \sum_{k=0}^{\infty} \frac{f^{(k)}(0)}{k!}x^{k}$. Because $\frac{f^{(k)}(0)}{k!}$ is constant, we can find the term using Least Square Estimator. If we find the constants which can makes $y=f(x)$, then we can reduce the cost function almost zero

 We can develop the above concepts into Hilbert space and Basis. In the square integrable functional space denoted as $L^2$ functional space where inner product could be defined, any function $f \in L^2$ function could be represented with basis $B_i(x)$ such as $f(x) = \sum_{i=1}^{\infty} \alpha_i B_i(x)$. Therefore, by mapping original feature $x$ into $B_i(x)$ and finding constant $\alpha_i$, we can find perfect predictive function. Such concepts could be use at a lot of statistics algorithms including Linear Regression, Principal Component Analysis and Support Vector Machine.

 The problem is, as expanding the data into higher space, the computational cost increase exponentially. For the linear regression $y = B_x\alpha$, the coefficient $\alpha_i$ could be estimated using LSE $(B_x^tB_x)^{-1}B_xy$. However, to get the estimation, we have to find the value of the matrix multiplication and inverse matrix which take a lot of cost. 

 We can shrink such cost by using the concept of Reproducing Kernel Hilbert Space. The more detailed explanation of RKHS is in the posting (https://hgmin1159.github.io/dimension/nonlineardr/).

 The important concepts to using the RKHS is as follow.

> For Reproducing Kernel Hilbert Space $H(\Omega)$ and its element $f$ and $g$, we construct following relation.
>
> - **Positive Definite Kernel:** Bivariate symmetric function $k(x_i,y_i) : \Omega \times \Omega \rightarrow \mathbb{R}$ is positive definite if for any finite subset $S \subset \Omega$, matrix $\{k(x_i,x_j)\}_{ij}, \quad (x_i ,x_j \in S)$ is positive definite matrix. 
> - **Basis** 
>   - Positive Definite Kernel with one fixed value such as $k_j(x_i) = k(x_i,y_j)$ is basis of RKHS.
>   - $f$ could be represented as $f = \sum_{i=1}^{p}\alpha_{fi} k_j(x_i), \quad (\alpha_{fi} \in \mathbb{R} )$ and the real valued vector $(\alpha_{f1},\alpha_{f2},...,\alpha_{fn})$ is called coordinate representation of $f$ and denoted as $[f]$
> - **Gram Matrix**: Matrix with basis $G = {k(x_i,x_j)}_{i,j} , \quad (i=1,2,...,n,j=1,2,...n)$ is called Gram Matrix
> - **Inner Product:** The inner product $\langle f,g \rangle $ could be defined as $\langle f, g \rangle = \sum_{i,j} [f]_i[g]_j k(x_i,x_j) =[f]^t G[g]$
> - **Reproducing Property: ** $\langle f, k_j\rangle = \sum_{i,j} [f]_i[k_j]_jk(x_i,x_j) = \sum_i [f]_ik(x_i,x_j) = f(x_j)$ $\because [k_j]_j = (0,0,...,1,...,0)$
> - **Kernel Trick:** For some specific vector valued function $\Psi = (\psi_1,\psi_2,...,\psi_p)$, there is a kernel $k(x,y)$ such that $\Psi(x)^t \Psi(y) = k(x,y)$

There are two famous functional vectors which could be calculated using the kernel trick;Gaussian Kernel $k(x,y) = \exp \left( -\frac{\parallel \textbf{x}-\textbf{x}' \parallel^2}{2\sigma^2}\right)$, Polynomial Kernel $k(x,y) = (x^ty+c)^d$.

To use the structure of the RKHS, the only thing we have to know is Gram matrix. Therefore, if we can get the gram matrix, then we can use them to build various non-linear statistical model. 



## 2. Quantum Kernel Estimator

 In the quantum computer, there is a algorithm to calculate the inner product between two qubits. Therefore, by using the algorithm, we can estimate the Gram Matrix of the quantum kernel and construct various quantum version of non-linear statistical model. Let's see the detailed procedure of estimation.

![figure1](\assets\img\post\2022-01-08\figure1.png)

We'll assume that each of U matrix maps into data vectors. That is,$U_{0}\ket{0} = \ket{x} , U_1 \ket{0} = \ket{y}$

Let track down the procedure. 

1. $(I \otimes U_0 \otimes U_1) \ket{000} = \ket{0xy}$
2. $(H \otimes I \otimes I) \ket{000} = \frac{1}{\sqrt{2}}(\ket{0}+\ket{1})\ket{xy}$
3. $\frac{1}{\sqrt{2}}CSWAP[(\ket{0}+\ket{1})\ket{xy}]  = \frac{1}{\sqrt{2}}(\ket{0xy}+\ket{1yx})$
4. $\frac{1}{\sqrt{2}}(H \otimes I \otimes I) (\ket{0xy}+\ket{1yx}) = \frac{1}{2}(\ket{0}(\ket{xy}+\ket{yx})+\ket{1}(\ket{xy}-\ket{yx}))$

5. Measurement
   - $Pr(0) = \frac{1}{4}\parallel \ket{xy}+\ket{yx} \parallel_2^2 = \frac{1}{2}(1+\mid\langle x,y \rangle \mid_2^2)$



By applying them, we can build general version of inner product estimator by setting encoding gates as kernel operator.  That is, we can estimates $k(x,y) = \langle \phi(x),\phi(y) \rangle $.



There are several advanced version of estimating kernel estimator but we'll not see them. 



## 3. Algorithms using Quantum Kernel

There are several strategy to use the quantum kernel.  We'll see three strategy to use the quantum feature space. I named them as Hybrid Kernel Model, Analytic Quantum Model with kernel encoding and Variational Quantum model with kernel encoding. The most representative algorithms are Quantum Kernel E, Distance-based Classifier and Parameterized Quantum Circuit respectively.

**Hybrid Kernel Model**

![figure2](\assets\img\post\2022-01-08\figure2.png)

 The first strategy is using the quantum computer only to get the Gram matrix and building models with classical computer. 

As mentioned above, the Gram matrix is the only things to use the reproducing kernel Hilbert space. Therefore, by estimating the gram matrix on the quantum computer and building classical model using the quantum gram matrix, we can use the quantum feature space. 

The procedure is simple. 

1. Constructing Quantum Kernel Estimator circuit.
2. Evaluating kernel values by iteratively input data vectors and constructing gram matrix.
3. Building classical model using the gram matrix. 



There are several classical model using the gram matrix including Support Vector Machine, Kernel Principal Components Analysis, K-Nearest Neighborhood and Generalized Sliced Inverse Regression. Therefore, by using the above procedure, we can build various quantum version of data science models. 

 There are several advantages of above algorithm. The first one is we can use quantum feature space. Even though they are in research, but the complex quantum space show peculiar characteristics compared to Gaussian kernel space or Polynomial kernel space because they are essential periodic. The more detailed research is in [Vojtěch Havlíček et al (2019) Supervised learning with quantum-enhanced feature spaces]. 

 The second one is we can get the result of machine learning model through classical computer. That is, the quantum computer is used only at the training phase. Once the models are trained, we can see the result using the classical computer only. For current quantum environment, it is not easy and cheap to use the quantum computer. Therefore, by above strategy, we can reduce the cost of quantum model because it use the quantum computer only at the training phase. 

 

**Analytic Quantum Circuit with Kernel Encoding**

<img src="\assets\img\post\2022-01-08\figure4.png" alt="figure4" style="zoom:50%;" />

 The second strategy is setting encoding operator which maps the raw data into quantum feature space. That is, we use the $E_n(x) \ket{0}=\phi (x)$ and injecting the distance estimator into operator $U$. By using them, we can build most intuitive algorithm and can control the total procedure. One of the famous algorithm is quantum distance based classifier. There are various paper about them including [Carsten Blank (2020) Quantum classifier with tailored quantum kernel].  

![figure5](\assets\img\post\2022-01-08\figure5.png)

 If we can get exact procedure to build the classifier, we can make most intuitive quantum model directly. But there is just a few algorithm to build supervised or unsupervised model. Those algorithms are under study.



**Variational Quantum Circuit with Kernel Encoding**

<img src="\assets\img\post\2022-01-08\figure3.png" alt="figure3" style="zoom:50%;" />

But there is more easy way to build the quantum model. If we use the parameterized quantum circuit, then we can build quite a good model. The one of the limit of parameterized quantum circuit is they cannot exploring nonlinear space. However, if we use the quantum kernel, then we can explore the nonlinear space to by mapping the data into nonlinear space with quantum kernel and finding most efficient linear mapping by parameterized gates. 

Therefore, the parameterized quantum circuit and quantum kernel show quite a good combination. There is a good example of PQC using kernel  encoding in [Tak Hur (2021) Quantum convolutional neural network for classical data classification]. 

<img src="\assets\img\post\2022-01-08\figure6.png" alt="image-20220110165801349" style="zoom:67%;" />

 It shows ideal result using quantum simulator. The paper introduced various ansatz and encoding methods and most of them shows high accuracy like 94~98%.



***

As we see, the quantum kernel shows very bright potential. Not only they can be easily implemented using the quantum computer, it might show us the new point of views about data science and data space. 



***

Vojtěch Havlíček et al (2019) Supervised learning with quantum-enhanced feature spaces

Carsten Blank (2020) Quantum classifier with tailored quantum kernel

Tak Hur (2021) Quantum convolutional neural network for classical data classification
