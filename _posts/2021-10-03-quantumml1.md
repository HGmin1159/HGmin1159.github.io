---
title: \[Quantum Data Science\] Part 1. Quantum Information Encoding
categories: [Quantum]
tags: [Quantum Computer]
excerpt: Encoding method to embed data into quantum states
sidebar:
  - title: "Quantum Computing"
    image: /assets/img/quantum.png
    image_alt: "image"
    nav: Quantum_eng
author_profile: False
---




 Compared to classical computing, quantum computing usually includes an additional step when solving data science problems. Before learning quantum circuits, we need to load the data into the circuits. The procedure is called as *state preparation*. 

 We need this step because the quantum version of RAM has not been invented yet. This step is also called quantum encoding and there are various quantum encoding methods. Different circuits take different strategies to encode the information. 

 In this posting, we'll see four different strategies to implement the data: Basis encoding, Amplitude encoding, Qsample Encoding, Hamiltonian Encoding. 
$\def\ket#1{\mid #1 \rangle}\def\bra#1{\langle #1 \mid}$

## 1. Basis Encoding

Let's assume that we have a binary dataset whose elements can be represented as  $x_i = \{b_{1i},b_{2i},...,b_{pi}\},i=1,2,...N$ . The first subscript indicates index of feature and the second one indicates index of sample. Therefore, the design matrix is $N \times P$.

 For them, the basis encoding prepares the density matrix as $\ket{D}\bra{D} = \frac{1}{N} \sum_{i=1}^N \ket{x_i}\bra{x_i}$. It can be interpreted as a superposition of each data point weighted by the total number of the same data. For example, let's assume the following data matrix.

| Index | $b_1$ | $b_2$ | $b_3$ | $b_4$ |
| ----- | ----- | ----- | ----- | ----- |
| 1     | 1     | 0     | 0     | 1     |
| 2     | 0     | 1     | 0     | 0     |
| 3     | 1     | 0     | 0     | 1     |
| 4     | 1     | 1     | 0     | 0     |
| 5     | 1     | 1     | 1     | 1     |

The basis encoded data matrix is as follows.

$$
\ket{D}\bra{D} = \frac{1}{5}\ket{1001}\bra{1001} + \frac{1}{5}\ket{0100}\bra{0100}+\frac{1}{5}\ket{1001}\bra{1001}+\frac{1}{5}\ket{1100}\bra{1100}+\frac{1}{5}\ket{1111}\bra{1111} \\\ \qquad  =\frac{2}{5}\ket{1001}\bra{1001} + \frac{1}{5}\ket{0100}\bra{0100}+\frac{1}{5}\ket{1100}\bra{1100}+\frac{1}{5}\ket{1111}\bra{1111}
$$

The state vector of it is below

$$
\ket{D} = \frac{\sqrt{2}}{\sqrt{5}} \ket{1001} +\frac{1}{\sqrt{5}} \ket{0100}+\frac{1}{\sqrt{5}} \ket{1100}+\frac{1}{\sqrt{5}} \ket{1111}
$$

As you can see, the number of possible states are $2^4 =16$ but only four of them are non-zero. That is, the amplitude vector is $\alpha = (0,0,0,0,\frac{1}{\sqrt{5}},0,0,0,0,\frac{\sqrt{2}}{\sqrt{5}},0,0,\frac{1}{\sqrt{5}},0,0,\frac{1}{\sqrt{5}})$.  Therefore, in the basis encoding, the amplitude vector becomes sparse or at least imbalanced.



 We can see the example of basis encoding at the quantum phase estimation. The result of the circuit is the phase represented as binary number such as $\ket{0100101011}$



## 2. Amplitude Encoding

 Amplitude Encoding is a much more famous encoding method than any other method. It is because it directly encodes the form of the vector. 
 
$$
\ket{\psi_D} = \underset{j=1}{\overset{P}{\sum}} \underset{i=1}{\overset{N}{\sum}}x_{ij}\ket{i}\ket{j}
\\\
\qquad = \underset{j=1}{\overset{P}{\sum}} \ket{\psi_{xi}}\ket{j}
$$

This could be interpreted as storing $x_{ip}$ value in the amplitude of the index state $\ket{i}\ket{j}$. Or, it could be interpreted as $vec(X_d)$.

The amplitude vector of it is $\alpha = (x_{11},x_{12},...,x_{1p},x_{21},...,x_{np})^t$. For them, we have to normalize the vector to make it satisfying $\mid \alpha \mid^2 =1$.

 If the target data is just vector, then it has same form with the vector itself.(Of course, the data vector have to be normalized first)

$x = \left[\begin{array}{c}x_1 \\\ x_2 \\\ \vdots \\\ x_N \end{array}\right] = \sum_{j=1}^{N} x_j \ket{j}$

For the $2^{n-1}< N \leq 2^n$, we can use n qubit and for remaining space $2^n-N$ could be encoded as zero which is called an ancilla state.

 Notice that for general state $\ket{\psi} = \sum \alpha_i \ket{i}$, if the data is encoded at $\alpha_i$, then it is called as an amplitude encoding and if the data is encoded at $\ket{i}$, then it is called as a basis encoding.

 The amplitude encoding has an advantage that only needs $n=\log NP$ qubits to encode the data matrix whereas the basis encoding needs $ n = P$ qubits.

 

 The amplitude encoding could directly implement some matrices or vectors. Therefore, it is generally used in linear algorithms including the HHL algorithm or qPCA.



## 3. Qsample Encoding

Qsample encoding is a method to connect a concept of probability distribution of a discrete random variable to the former state form. 

Assume there is a pmf of random variable X as $Pr(X=i) = p_i$. Any discrete random variable could be represented like it just by indexing the events. For them, we can define qubit state as $\ket{\psi} = \underset{i=1}{\overset{N}{\sum}} p_i \ket{i}$. 

In the view of Qsample encoding, the basis encoding could be understood as an empirical distribution function because the amplitudes of the basis encoding are the weights of the data which could be interpreted as empirical probabilities.

 

 By using the Qsample encoding, we could express a joint probability of several random variables too.

For states of two random variables $\ket{x} = \underset{i=1}{\overset{2^n}{\sum}} \sqrt{Pr(x=i)} \ket{i}$ and $\ket{y} = \underset{j=1}{\overset{2^n}{\sum}} \sqrt{Pr(y=j)} \ket{j}$, the joint state of two random variable is $\ket{x,y} = \underset{i,j}{\overset{2^n}{\sum}} \sqrt{Pr(x=i)Pr(y=j)}\ket{i}\ket{j}$ if two random variables are independent and  $\ket{x,y} = \underset{i,j}{\overset{2^n}{\sum}} \sqrt{Pr(x=i,y=j)}\ket{i}\ket{j}$ if two random variables are dependent each other.

 

We can observe that the definition could directly express the random variable. Even though a random variable is a kind of function, we usually interpret it as an instance which emits some real value with respect to some probability. In this manner, the random variable could be interpreted as a qubit because both of them could return a specific real value with measurement (observation) and before we observe them, they are potential instances which could be any real value.

  By developing the concept, we might be able to build a special statistical model.



## 4. Hamiltonian Encoding

 The Hamiltonian Encoding is a little bit different from previous methods. In the previous methods, we encoded the data in the states of qubit. Meanwhile the hamiltonian encoding method encodes the data into the operator.

 To make some matrix A to Hamiltonian, we have to make it Hermitian first. Because the definition of Hermitian is $H = H^{\ast}$, we can make it by $H = A^\ast A \quad(H^\ast = A^\ast A)$ 

 Then to make it as a unitary matrix, we can use a matrix exponential as $e^{-iAt}$. 

There is a theorem that any hermitian matrix could be decomposed with Pauli gates. 

$$
\mbox{ For any unitary matrix } A, \mbox{ there is a real vector }(\alpha,\beta,\gamma,\delta) \mbox{ such that } A = \alpha+\beta X + \delta Y + \gamma Z
$$

By using it, we can develop unitary evolution as follows.

$$
e^{i A t} = e^{i(\alpha + \beta X + \delta Y + \gamma Z)t} \\\ = e^{i\alpha t I} e^{i\beta t X} e^{i \delta t Y} e^{i\gamma t Z} \\\ = R_x(\beta t) R_y(\delta t) R_z( \gamma t)
$$

That is, we can express any unitary evolution by three rotation operators. 



However, in reality, it cannot be done. This is because the commutative rule does not hold for matrices. That is, $AB\neq BA$ so $e^{A+B} \neq e^{A}e^{B}$. 

However, we can overcome this by using the Trotter Suzuki formula.

$$
\mbox{ For large enough r, the following equation holds } e^{-i(H_1+H_2)t} \approx (e^{-i H_A t/r}e^{-i H_B t/r})^r
$$

That is, we can implement some hamiltonian unitary matrices by finding the rotating magnitudes and rotating the state with them gradually.



 The hamiltonian encoding method is densely connected with Physics. Therefore, various physical algorithms including VQE and QAOA are developed with hamiltonian encoding methods.

***

Schuld, M. & Petrucionne, F. (2019). Supervised Learning with Quantum Computers,Cham, Switzerland:Springer

---
title: \[Quantum Data Science\] Part 1. Introduction to Quantum Theory
categories: [Quantum]
tags: [Quantum Computer]
excerpt: Basic Terminology used at the Quantum Theory
sidebar:
  - title: "Quantum Computing"
    image: /assets/img/quantum.png
    image_alt: "image"
    nav: Quantum_eng
author_profile: False
---

 In this posting series, I will introduce the newest quantum machine learning algorithms such as QAOA(Quantum Approximated Optimization Algorithm), QKE(Quantum Kernel Estimator) and QPCA(Quantum Principal Component Analysis).

 However, all of these papers come from physicists who are experts on quantum theory. Therefore, they use terminologies in quantum physics which are not familiar to us. Thus, we need to study the concepts first before watching the papers. In this posting, I'll introduce the basic concepts of quantum physics. 

 This posting is based on the book "Supervised Learning with Quantum Computers" by Schuld. M and Petruccione. F 

# 1. Quantum Theory

**Quantum Theory**

 Quantum Theory, which is more famous to be known as Quantum Mechanics, is 'first and foremost a calculus for computing the probabilities of outcomes of measurements made on the physical system'. Unlike classical mechanics which aims to explain the situation as it is, quantum theory aims to explain the potential situation and cause too. 

 It is useful because it can explain observations which cannot be explained by Classical mechanics. However, because it is too complicated and difficult and in some portions it even cannot be understood, quantum theory is not being able to replace the whole mechanic theory.

 According to the book, quantum theory can be understood as a generalization version of classical probability theory which regards the outcome as a result of measurement. In classical probability theory, we use probability measure theory in real analysis (we have to discriminate between "measure" and "measurement". The former one indicates a special function in mathematical analysis and the later one indicates an experimental evaluation in physics)  and explain the probability as magnitude of some set in Borel sigma algebra which can be understood as a set of all possible experimental outcomes. Then, we can understand that the quantum information theory goes deeper than probability measure theory and analyze the measure with superposition and entanglement.



**State**

 Assume there is a set of events (or observation) $S = \{s_1, s_2, ..., s_N\}$ . Then we can define a random variable X which takes a real valued vector corresponding to the event $\{x_1,x_2,...x_N\}$.  Remark that X is defined as a function which maps real events to real values (the definition could be more generalized). For each real valued vector, we can define a probability measure as $\{p_1,p_2,...p_N\}$ which satisfies $\sum_{i=1}^{N} p_i =1$. Then we can define a expectation value of random variable X as $\langle X \rangle = \sum_{i=1}^{N} p_i x_i$ (Remark that we restricted the codomain of X as a real scalar field)

In Quantum Theory, we generalize the concepts. In the classical expectation, $p_i$ is in real scalar (0,1). Let's generalize it into complex scalar $\sqrt{p_i}$. And define following vector and matrix

$q = \left[\begin{array}{c} \sqrt{p_1}\\\ \sqrt{p_2}\\\ \vdots \\\ \sqrt{p_N} \end{array}\right]$, $X = \left[ \begin{array}{ccc} x_1 & \cdots & 0 \\\ \vdots & \ddots & \vdots \\\ 0 & \cdots & x_N \end{array}\right]$

Then we can rewrite the expectation value as $\langle X \rangle = q^t X q = \sum_{i=1}^{N}p_i x_i$

As we use the matrix and vectors to indicate the probability and event, by using the basis like assigning each event to standard basis $e_i =:x_i$, we can make it simpler further like $O = \sum e_i e_i^t$.  Furthermore, in the complex field, there are infinite values whose squares are some constant probabilities. Therefore, to be exactly speaking, $q$ become $\alpha = \{\alpha_1,\alpha_2,....,\alpha_N\} \quad (\alpha_i^2 = p_i)$ 

The $\alpha$ vector is called as **amplitude vector** and $O$ is called as **Observable** which have to be a self-adjoint matrix. The observable matrix has to be a hermitian matrix which is a complex generalized version of the symmetric matrix. 

Finally, we say the linear combination of amplitude vector and event basis as **state** which is represented as $\sum_{i=1}^N \alpha_i e_i$



**Unitary Evolutions**

 In the quantum physics, we are generally interested in the changes of some state which is called as **Evolutions**. Each change can be perfectly represented with linear algebra because they are all discrete values. Remark the probability transformation matrix (or called as Stochastic Matrix) of Markov Process. 

$$MP = \left[ \begin{array}{ccc} m_{11} & \cdots & m_{1n} \\\ \vdots & \ddots & \vdots \\\ m_{n1} & \cdots & m_{nn} \end{array}\right]\left[ \begin{array}{c} p_{1}\\\ \vdots \\\ p_{n} \end{array}\right]=\left[ \begin{array}{c} p'_{1}\\\ \vdots \\\ p'_{n} \end{array}\right]$$

This means that the event in time t with probability $\{p_1,p_2,..p_n\}$ has the probability of $MP$ at time t+1. For them, there need constraint that sum of probabilities have to be one both before and after.($\sum_i^n p_i = \sum_i^n p_i' =1$) To do them, each sum of column in stochastic matrix have to be one. 

 We can generalize them into quantum evolution.

 $$U\alpha = \left[ \begin{array}{ccc} u_{11} & \cdots & u_{1n} \\\ \vdots & \ddots & \vdots \\\ u_{n1} & \cdots & u_{nn} \end{array}\right]\left[ \begin{array}{c} \alpha_{1}\\\ \vdots \\\ \alpha_{n} \end{array}\right]=\left[ \begin{array}{c} \alpha'_{1}\\\ \vdots \\\ \alpha'_{n} \end{array}\right] = \alpha'$$

For these notation, we need constraint $\sum_i^n \mid \alpha_i \mid^2 = \sum_i^n \mid \alpha_i' \mid^2 =1$

To do meet the restriction, the U have to be unitary matrix which means $U^\ast U = U U^{\ast}=I$

>  Brief confirmation of it is as follows.
>
> $\alpha^{\ast} \alpha = \sum_i^n \mid \alpha_i \mid^2=1$
>
> $\alpha'^\ast \alpha' = \alpha^\ast U^\ast U \alpha =1 ,\forall \alpha \mbox{ such that }\alpha^\ast \alpha =1$
>
> By injecting $e_i$ as $\alpha$, we can analyze $U^\ast U = I$ which means unitary matrix. 

Because any transformation operator in quantum theory has to be unitary, we could know that $U^{-1}=U^{\ast}$ which means that to reverse the state, all we need to do is apply the conjugate transpose of the operator.



In addition to the unitary operator, there is another operator to change the state. This is called as **measurement** which makes the probability to either one or zero.



**Density Matrices**

 If we perfectly know the amplitude vector $\alpha = (\alpha_1 , \alpha_2)^t$ of some state, then the state is called as *pure* state. However, if we are uncertain that our state follows $\alpha$ or $\beta$, then the state is called as *mixed* state. Remark that we call the distribution which consists of several PDFs as Mixture. 

 In the physics, they usually represent amplitude vector in the form of matrix called as **density matrix**. A density matrix $\rho$ corresponding with pure quantum state $\alpha$ is given by the outer product of the vector $\rho = \alpha \alpha^\ast$. According to the book, this is similar to the covariance matrix. 

 In the mixed state, which we do not know the exact state of, we can represent a mixed state like $\rho = p_1 \alpha \alpha^\ast + p_2 \beta \beta^\ast$. 

$\def\ket#1{\mid #1 \rangle}$

## 2. The Postulates of Quantum Mechanics

Now, we'll use Dirac notation. Basic Dirac Notation is in the previous posting. 

**Observables**

As mentioned before, they have to be self-adjoint matrices a.k.a hermitian matrix. Every hermitian matrix can be decomposed as follows $O = \underset{i}{\sum} o_i \ket{e_i} \langle e_i \mid $ which is known as spectral decomposition.

 There is a theorem that $f(O) = \underset{i}{\sum}f(o_i)\ket{e_i} \langle e_i \mid$. 

> $f(O) = \underset{k=0}{\overset{\infty}{\sum}}\frac{f^{(k)}(0)}{k!}O^k \\\ \qquad = \underset{k=0}{\overset{\infty}{\sum}}\frac{f^{(k)}(0)}{k!} \underset{i}{\sum} o_i^k \ket{e_i} \langle e_i \mid  \\\ \qquad =  \underset{i}{\sum} \underset{k=0}{\overset{\infty}{\sum}}\frac{f^{(k)}(0)}{k!}  o_i^k \ket{e_i} \langle e_i \mid \\\ \qquad = \underset{i}{\sum} f(o_i) \ket{e_i} \langle e_i \mid$

For any analytic function f, we can use the theorem. 

The most important f is $f(A) = exp(i\epsilon A)$ because for any hermitian matrix H, $exp ( i \epsilon H)$ is an unitary matrix. $exp(i\epsilon H)^{\ast}exp(i\epsilon H) = exp(-i\epsilon H +i \epsilon H) = exp(0)=I$



The expectation values of quantum mechanical observable O can be calculated as follows. 

$\langle O \rangle = tr(\rho O)$ which is a trace of multiplication of the observable and the density matrix. 

It can be rewritten as following form if $\rho = \ket{\psi} \langle \psi\mid$

â€‹        $= tr(\ket{\psi} \langle \psi\mid O ) = tr(\langle \psi\mid O  \ket{\psi}) = \langle \psi\mid O  \ket{\psi}$

The latter form is more famous than the former form.



**Time Evolution**

 There is a famous equation in quantum physics: Schrodinger equation.
$$
ih \frac{d}{d t} \ket{\psi} = H \ket{\psi}
$$
 It represents the time evolution of a quantum mechanical system. H denotes *Hamiltonian* which represent the changes of some quantum and h is a Planck's constant. 

By solving the differential equation, we can derive the solution of $\ket{\psi(t)}$ when the hamiltonian is time-independent as follows.
$$
\ket{\psi(t)} = exp(-i \frac{t}{h}H) \ket{\psi(0)}
$$
 which shows that if we know the initial state $\ket{\psi(0)}$ and its movement H, then we can know the exact state of $\psi$ after time t goes. 

> $\frac{d}{dt} \ket{\psi(t)} = -i\frac{1}{h}H \exp(-i\frac{t}{h}H)\ket{\psi(0)}$
>
> $ih \frac{d}{dt}\ket{\psi(t)} = H exp(-i \frac{t}{h}H) \ket{\psi(0)} = H \ket{\psi(t)}$



There is a similar equation which is called the Von Nuemann equation.
$$
ih \frac{d}{dt} \rho(t) = \frac{d}{dt} \underset{i}{\sum}p_i \ket{\psi_i}\langle \psi_i \mid 
\\ =[H , \rho] 
$$
where $[A,B] = AB-BA$  which means commutator. 

The equation can be called a density matrix version of Schrodinger equation.

And its solution is $\rho(t) = \exp(-i\frac{t}{h}H) \rho(t_0) \exp(i\frac{t}{h}H)$

By using the notation, we can transform the state into just a smaller portion of it. 



**Quantum Measurement**

For some observable of specific event $P_i$, to measure the probability of some state, we can use *Born Rule*.
$$
Pr(o_n) = tr(P_n \rho) \\ = tr(P_n \ket{\psi}\langle \psi \mid)\\ = \parallel P_n \ket{\psi} \parallel^2
$$
By using them we can derive the probability that some states have some specific results.



After measuring the state, the state become $\ket{\psi} \rightarrow \frac{P_n\ket{\psi}}{\sqrt{\langle\psi\mid P_n \ket{\psi}}}$ and in density matrix form, it becomes $\frac{P_n \rho P_n}{tr(P_n \rho)}$

***

Even after studying this unfamiliar notation, there are other fundamentals which have to be studied. In the next posting, we're gonna see the information encoding method of quantum computing.

***

Schuld, M. & Petrucionne, F. (2019). Supervised Learning with Quantum Computers,Cham, Switzerland:Springer

