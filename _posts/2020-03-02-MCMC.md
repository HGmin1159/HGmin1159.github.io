---
title: \[베이즈통계\]베이지안 인퍼런스 - MCMC
categories: [Bayesian]
tags: [베이지안,Bayesian]
excerpt: MCMC방법인 Gibbs Sampling과 MH 기법에 대한 정리
sidebar:
  - title: "Bayesian Statistics"
    image: /assets/img/bayes.png
    image_alt: "image"
    nav: Bayesian
    author_profile: False
---

## 1. 개요

베이즈 통계학에서는 다음과 같은 수식을 통해 데이터 업데이트의 개념을 사용해 데이터가 반영된 모수 분포를 추정하는 것을 목표로 한다.

### $p(\theta \mid D) = \frac{p(D \mid \theta) \times p(\theta)}{p(D)}$

그런데 $p(D) = \int p(D \mid \theta) p(\theta) d \theta$ 부분의 경우 추정 대상 모수가 늘어남에 따라 막대한 계산량이 동반되기 마련이다. 따라서 베이즈 통계학에서는 우회로를 통해 $p(\theta \mid D)$를 추정해내는데 그 방법 중 하나가 바로 MCMC이다. 

## 2. MCMC 란?

MCMC(;Markov Chain Monte Carlo)란 마르코프 체인의 성질을 이용한 시뮬레이팅 기법으로 $p(D)$를 찾아내지 않고도 $p(\theta \mid D)$를 찾아낼 수 있는 방법이다.  

마르코프 체인의 성질에 따르면 Transition Probability Matrix가 Detailed Balance Condition을 만족할 시 n이 충분히 클 때 특정한 한 분포로 수렴한다는 것을 알 수 있다. 이때 Transition Probablity Matrix를 베이즈 룰의 Density부분으로 설정할 시 Detailed Balance Condition을 만족시키면서 Stationary Distribution이 $p(\theta \mid D)$인 확률과정 문제로 만들 수 있다. 이때 확률과정의 샘플링을 충분히 많이 한다면 샘플들의 분포는 모수 분포를 따르게 될 것 이다.

## 3. Metropolitan-Hasting Algorithm

 모든 MCMC 알고리즘은 Proposal을 하는 부분과 Acceptance를 하는 부분, 둘로 나뉘어져 있다. Proposal은 다음 샘플링을 할 후보군을 뽑는 과정이고 Acceptance는 이 후보군에 대한 transition probability를 반영하는 부분이다.  

확률 과정 $X_t$에 대해 $p(X_{t+1} =  s' \mid X_t = s)$를 trainsition probablity라고 할 때 아무 $s'$만 평가하는 것이 아니라 $k(s' \mid s)$를 이용해 $s'$를 제안하고, accept할지 말지를 결정함으로써 Detailed Valance Condition을 만족하는 확률과정 문제로 만들어 낼 수 있다. 

구체적인 과정은 다음과 같다.  

1. 샘플링 후보 $s'$를 $k(s' \mid s)$ (s는 이전 시점의 states)로 부터 뽑아낸다.  
2. min(1,$p(X_{t+1} =  s' \mid X_t = s)$)의 확률로 후보 $s'$를 샘플링 풀에 편입시킨다. 
3. 편입되지 않을 시 s가 한번 더 샘플링 풀에 들어간다. 
4. 1~3을 충분히 반복시 stationary distribution을 띄는 샘플들을 얻어 낼 수 있다.  

이때 transition probability를 $p(X_{t+1} =  s' \mid X_t = s) = \frac{p(D \mid s') p (s')}{p(D \mid s) p (s)}$로 설정해줄시 stationary distribution은 $p(s \mid D)$를 띄게 된다.  



## 4. Gibbs Sampling

Gibbs Sampling 또한 MCMC의 일종이다. Metropolis-Hasting 알고리즘과 다른 점은 Full Conditional Distribution이 Proposal과 Acceptance 역할을 모두 한다는 것에 있다.  

Full Conditional Distribution은 추정해야 하는 모수가 여러개 일시 ($\theta = (\theta_1,\theta_2,...,\theta_p)$) 나머지 모든 모수와 데이터가 주어져 있을때의 모수 분포를 의미한다. 즉, $\theta_1$의 F.C.D는 $p(\theta_1 \mid D , \theta_2, ...,\theta_p)$이다.  

Gibbs Sampling 과정은 다음과 같다. 

1. $p(\theta_1 \mid D , \theta_2, ...,\theta_p)$에서 $\theta_1$을 뽑아낸다. 
2. 뽑아낸 $\theta_1$을 $p( \theta_2 \mid D , \theta_1,...,\theta_p)$에 반영한 후 $\theta_2$를 뽑아낸다. 
3. 나머지 모든 모수에 대해 1,2를 반복한 뒤 다시 $\theta_1$부터 뽑아낸다. 

F.C.D를 모두 유도 해낼 수 있다는 가정하에서 Gibbs Sampling은 MCMC에 비해 다변량에서 더 유리하다는 장점이 있다.  





MCMC를 통한 베이지안 모수 추정법은 앞서 살펴본 Variational Inference에 비해 느리지만 정확하다는 장점이 있다. 또한 Variational Inference가 Optimization 방법이라면 MCMC는 Sampling에 기반을 둔 방법이다. 둘 모두 베이지안 인퍼런스를 하면서 필수적이므로 잘 숙지해놓을 필요가 있다. 
