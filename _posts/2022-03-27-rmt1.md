---
title: \[Random Matrix Theory\] Ch1. Probability Theory
categories: [RMT]
tags: [random matrix, probability, Mathematics]
excerpt: Preliminary of random matrix theory.
sidebar:
  - title: "Random Matrix Theory"
    image: /assets/img/RMT.png
    image_alt: "image"
    nav: RMT
author_profile: False
---



 The random matrix is literally the matrix whose entries are random variables. It has various application including high dimensional statistics, information theory and quantum dynamics. Moreover, according to [Jinho Baik et al (2016) Combinatorics and Random Matrix Theory], it is also connected with combinatorics. 

 Therefore, I believe that if I study the random matrix theory, I can get new intuition about my current fields of interest such as dimensional reduction and quantum computing. Maybe it could help to study combinatoric optimization too. 

 Text books about Random Matrix Theory written by Jinho Baik target for experts on mathematics or physics. So I will read them after I become familiar with the theory. I'll use the text book [Terence Tao (2012) Topics in Random Matrix Theory]. It is more appropriate for me in that it handles the random matrix theory in statistical background. 

  The first chapter of the book contains probability theory in purer fields. So it can help me to get familiar with purer concepts of probability theory such as "almost sure" or "Probabilistic concepts". Let's start it.



# Ch1. Probability Theory

## A. Probabilistic Concepts 

 In the construction level, the probability theory could be viewed as theory about measure space whose total measure is one. 

We can construct the probability space as triplet $$(\Omega,\mathcal{B},P)$$. 

> **Sample Space** $$\Omega$$: A set containing all the possible outcomes of random situation where one is concerning.  
>
> **Sigma Algebra** $$B$$ of $$\Omega$$:  A set of subsets of the sample space. There are several rules in sigma algebra (omitted)
>
> **Event**: The element of the sigma algebra
>
> **Probability Measure** $$P$$: A function assigning real numbers between 0 and 1 to each event. 

 Remark that the elements of the sigma algebra is called as measurable sets at text book of Real Analysis but in the context of probability theory, it is specially called as events. An element of the sample space is usually denoted as $$\omega$$.

 For example, suppose there is a coin and we are studying the pattern of head and tail of the coin. Then the sample space is $$\{ t,h\}$$. And sigma algebra containing every subsets is $$\{\phi,\{t\},\{h\},\{t,h\}\}$$. Then the function $$f$$ mapping each element of sigma algebra to real value is probability measure. 



 We can **extend** the probability space $$(\Omega, \mathcal{B}, P)$$ into another probability space $$(\Omega',\mathcal{B}',P')$$ if there is a surjective mapping $$\pi : \Omega \rightarrow \Omega'$$ which is measurable i.e. $$\forall E \in \mathcal{B}', \pi^{-1}(E) \subset \mathcal{B}$$ and preserves probability i.e. $$\forall E \in \mathcal{B}', P(\pi^{-1}(E))=P(E)$$. The author of the book states that there is a important dogma: "probability theory is only 'allowed' to study concepts and perform operations which are preserved with respect to extension of the underlying sample space." In this manner, the probability use different notations with usual set theory. 

> $$E \or F:$$ The union of events $$E$$ and $$F$$. The event that at least one of $$E$$ and $$F$$ holds
>
> $$E \and F :$$ The intersection of events $$E$$ and $$F$$. The event that $$E$$ and $$F$$ both hold.
>
> $$\overline{E} :$$ The event that $$E$$ does not hold or the event that $$E$$ fails.
>
> $$E \subset F :$$ E is contained at F.

This notation is similar with logical operator of boolean algebra. 

 

To construct analytic structure of the probability theory, we need special boundary theory. 

> $$X=O_k(Y) :$$ If $$\mid X \mid \leq c_kY$$ for some scalar $$c$$ depending on some parameter $$k$$. 
>
> $$X = \Omega_k(Y)$$ : If $$\mid X \mid \geq c Y$$ for some scalar $$c$$ depending on some parameter $$k$$.
>
> $$X = o_k(Y): $$ If $$\mid X \mid \leq c_k Y$$ for some $$c$$ that goes to zero as $$k$$ goes to infinity.



In deterministic theory, each proposition is one of two: True or False. However, because things in probability theory are handled in random situation, it is hard to say that some properties holds 100% true or false. Therefore, we use true and false in probabilistic sense as follow.

> **Logical statement in probabilistic sense.**
>
> 1. **Sure**: A proposition holds **surely** if the event of the proposition equals with total sample space.  
> 2. **Almost Sure**: A proposition holds **almost surely** if the probability of the event of the proposition is one. That is, the complement of the event is measure zero.
> 3. **Overwhelming probability** : A proposition holds with **overwhelming probability** if for every fixed $$\epsilon >0,$$ the proposition holds with the probability **Overwhelming probability** : A proposition holds with **overwhelming probability** if for every fixed $$\epsilon >0,$$ the proposition holds with the probability $$1-O_\epsilon(n^{-\epsilon})$$. (i.e. the probability that the event does not occur is below $$c_\epsilon n^{-\epsilon}$$)
> 4. **High probability** : A proposition holds with **high probability** if for some $$\epsilon >0,$$ the proposition holds with the probability $$1-O(n^{-\epsilon})$$. (i.e. the probability that the event does not occur is below $$c n^{-\epsilon}$$)
> 5. **Asymptotically almost surely** : A proposition holds asymptotically almost surely if it holds with the probability $$1- o(1)$$ (i.e. the probability that the event does not occur goes to zero as $$n$$ goes to infinity)

 We can see that except (5), the higher statement is stronger than lower one. 



## 2. Random Variable

  Even though the structure of probability theory is quite realistic and theoretically solid at the same time, because the event, the basic component of probability theory, can only be at two states, the theory is lack of flexibility. To supplement such lack of flexibility, we can use function mapping events into more general concepts. This is called as a random variable. 

> **Definition [Random Variable]**
>
> Let $$R = (R,\mathcal{R})$$ be a measurable space. A **Random Variable** is a measurable mapping from the sample space to $$R$$, i.e. $$X: \Omega \rightarrow R$$  such that $$X^{-1}(S)$$ is measurable set for $$S \subset \mathcal{R}$$. 

 The most usual target space $$R$$ is a real line $$\mathbb{R}$$, but we can use much more various target space. The subclass of the random variable could be defined by the type of the target space.



> **Subclass of the random variable**
>
> 1. **Discrete Random Variables** : If the target space $$R$$ is countable and $$\mathcal{R}$$ is $$2^R$$ which means set of every possible subsets of a countable set. Particularly, if the target space is $$R = \{0,1\}$$, we call the random variable as **Boolean** and if the target space consists with just one element, $$R = \{c\}$$, then we call it as **deterministic**. 
> 2. **Real-valued Random Variable** : If the target space $$R$$ is the real line $$\mathbb{R}$$ and $$\mathcal{R}$$ is Borel sigma algebra.
> 3. **Complex-valued Random Variable :** If the target space $$R$$ is the complex plane $$\mathbb{C}$$ and $$\mathcal{R}$$ is Borel sigma algebra
> 4. **Function of random variable:** For a measurable function $$f$$, a function onto random variable $$f(X)$$ is definitely random variable by definition of measurable function and random variable. $$i.e. X^{(-1)}\circ f^{-1}(S)$$ is definitely measurable for measurable set of domain of function. 
> 5. **Joint Random Variable :** By joining several random variable and making set as $$(X_\alpha)_{\alpha \in A}$$. Then the target space become cartesian product of each target space $$\bigotimes_{\alpha \in A} R_{\alpha}$$. There is no restriction on it that we can design various joining random variable such as $$Z = (C,D,X) , R_Z = R_C \otimes R_D \otimes R_X = \{\mbox{head},\mbox{tail}\} \otimes \{1,2,3,4,5,6\}\otimes \mathbb{R}$$.
> 6. **Vector-valued Random Variable :** It the target space $$R$$ is the finite dimensional vector space $$\mathbb{R}^d$$ or $$\mathbb{C}^d$$. 
> 7. **Matrix-valued Random Variable :** If the target space $$R$$ is a matrix space $$M_{p \times q}(\mathbb{R}) $$ or $$M_{p \times q}(\mathbb{C})$$.



Each random variable $$X$$ has its on distribution $$\mu_X$$ defined as $$\mu_{X} (S) := P(X \in S)$$. The important fact is every random variable has its own distribution and every kind of distribution can be connected with random variable. Therefore, we can regard two concepts has unique connection that if we know all about the random variable, it means we know the distribution of it and vice versa.

 If the random variable is discrete, then the distribution could be expressed as $$\mu_X(S) = \sum_{x \in S} p_x, \mbox{ (where } p_x := P(X=x))$$. There are famous discrete random variables.

> **Discrete Random Variable**
>
> 1. **Dirac distribution** $$\delta_a$$ : For $$x=a,p_x = 1$$  and for $$x \neq a , p_x =0$$.
> 2. **Discrete Uniform Distribution** : The target space $$R$$ is countable and $$p_x = 1/ \mid R \mid$$ for all $$x \in R$$.
> 3. **(Unsigned) Bernoulli Distributions** : The target space $$R$$ is $$\{0,1\}$$ and $$p_1 = p, p_0 =1-p$$ 
> 4. **Signed Bernoulli Distributions** : The target space $$R$$ is $$\{-1,1\}$$ and $$p_+ = p_- =1/2$$ 
> 5. **Lazy signed Bernoulli Distribution :** The target space $$R$$ is $$\{-1,0,1\}$$ and $$p_+ = p_- =\mu/2$$ and $$p_0 = 1-\mu$$ 
> 6. **Geometric Distribution** : The target space $$R$$ is $$\{0,1,2,...\}$$ and $$p_k = (1-p)^kp$$.
> 7. **Poisson Distribution** : The target space $$R$$ is $$\{0,1,2,...\}$$ and $$p_k = \frac{\lambda^k e^{-\lambda}}{k!}$$.
